# Simulating-OVB-with-Linear-Models
In this code I simulate a simple regression model to test the performance of a linear model when omitting an important variable. 

Empirical Monte Carlo
How Does the Density of Beta 1 and Beta 2 Change?
Under the first procedure, the density of beta 1’s becomes less peaked with fatter tails as correlation between the x1 and x2 variables increases. This is caused by the multicollinearity as the model has a harder time separating the effects of x1 and x2 on y. This leads to less precise estimates of beta 1 and beta 2 and consequentially less density around the true b1 and true beta 2. In the first procedure the effect of changing the variance of the error term has a similar effect. The density of beta 1 remains centered around 1, the true value of beta 1, but it becomes less peaked and exhibits fatter tails. Increasing variance in the error term leads to a wider variety of estimates for beta 1 and beta 2, so the models become less reliable. 
In the second procedure the density of beta 1 becomes more biased upward when x2 is excluded from the model and this bias becomes more pronounced as correlation between the regressors increases. This effect is caused by the correlation between x1 and x2. Leaving x2 out of the model when it is highly correlated with x1 means that more of the effect that x2 is having on y is being captured by beta 1. As the regressors become more correlated the estimates of beta 1 increase as omitted variable bias becomes more pronounced. When comparing the density of beta 1 across the values for p the density shifts right as the correlation increases. When variance in the error term increases but correlation between x1 and x2 is held constant the density of beta 1 doesn’t shift closer to two like it does when changing variance. Increasing the magnitude of errors leads to a flatter less peaked density of beta 1, but it does not become more biased as variance in the error term increases. 
In the third procedure the density of beta 1 becomes more concentrated around 2 as variance in the error term increases and correlation between x1 and x2 increase. When variance in the error term is increased, the model becomes less reliable and the statistical significance of the estimates of beta 1 and 2 is reduced. Since the algorithm drops beta 2 when it has a t score less than 2 more beta 2s are rejected and x2 is left out of a higher proportion of the models. When x2 is left out of the model beta 1 becomes biased upward as it starts to capture the effect of x2 on y. The degree of this bias depends on the correlation between regressors. When error term variance is held constant and correlation between x1 and x2 increase the bias on x1 increases and the density shifts further to the right. When x2 is dropped beta 1 is capturing the effect of x2 on y and it captures more of this effect as the regressors become more correlated. 
How does the Density of Beta 2 Change in Procedure 3?
In procedure 3 x2 is dropped when it beta 2 is not statistically significant (t score<2). For simulations where x1 and x2 are more highly correlated and there is more variance in the error term there’s a higher proportion of beta 2s with t scores below 2 that get dropped from the model.  The density of beta 2 generated by procedure 1 and procedure 3 are very similar when x1 and x2 are weakly correlated and there is low variance in the error term. As error term variance increases and correlation increases the beta 2s that are kept in the model have higher magnitudes than the beta 2s when x2 is dropped from the model. Because of multicollinearity the model has a harder time distinguishing the effects of x1 and x2 on y, this leads to the estimates of beta 2 that are statistically significant having higher values than the beta 2s that are less significant. In these cases, the model is attributing the effect of x1 on y to x2 as x1 and x2 become more correlated and it becomes more difficult to separate these effects. This leads to the model generating statistically significant estimates for beta 2 that are biased upward. With higher correlation the model can’t disentangle the effects of x1 and x2 on y. When the model attributes some of x1s effect on y to x2, beta 2 becomes more statistically significant and x2 is left in the model.  
How are t-statistics Distributed Under Different Models?
	In the first procedure, increasing correlation between x1 and x2 creates less reliable estimates for beta 1. With correlation making it more difficult for the model to disentangle the effects of x1 and x2 on y, the model exhibits multicollinearity. When there is more correlation in procedure one, we fail to reject the null more often. Increasing the variance of the error term has the same effect where we can’t reject the null as often when there is more variance in the error term. In procedure 2, increasing correlation improves the significance of the estimates of beta 1, allowing us to reject the null in more cases. Because x1 and x2 are more correlated omitted variable bias allows beta 1 to capture the true effect of x2 on y from the true data generation process. This leads to better model fit and less variance in the estimate of beta 1, as it shifts to the right and explains more of the variance in y. Increasing error term variance has the same effect as in procedure 1, we fail to reject the null in more cases as error term variance increases. In procedure 3, when variance is low more of the x2s remain in the model, the density of the t scores shifts to the left when regressors become more correlated as multicollinearity makes it difficult for the model to separate effects of x1 and x2 on y. Increasing variance of the error term increases the proportion of x2s that are dropped. When x1 and x2 are highly correlated and a higher proportion of x2s are dropped from the model the distribution of t statistics is similar compared to procedure 2 but we fail to reject the null for beta 1 in more cases when x2 is left in the model. 
How is R-Squared Distributed Across the Models?
	In procedure 1, increasing variance leads to worse model fit, but higher correlation leads to higher R squared. Seeing estimates of beta 1 with low statistical significance and higher r squared when we increase correlation between the regressors implies multicollinearity in the model. The model is able to explain variation in y but not able to reliably attribute it to x1 or x2. In procedure 2, increasing correlation between the regressors improves the fit of the model at all levels of error term variance. The models can more effectively fit the data when the correlation is high and x2 is left out as we don’t have multicollinearity reducing the statistical significance of our estimates of beta 1 and x2s effect on y is more effectively captured by beta 1. Error term variance decreases model fit in procedure 2 as in procedure 1. In procedure 3, when variance is low fewer x2s get dropped from the model and there is a multicollinearity problem that causes higher r squared as correlation increases. As error term variance increases more x2s are dropped and model fit improves as regressors become more correlated. Some of this is multicollinearity as not all x2s are dropped from the model but under high correlation the procedure is more able to fit the data and have statistically significant beta 1s due to x2s being dropped when variance is higher. 

